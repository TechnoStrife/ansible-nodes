---
grafana_security:
  admin_user: admin
  admin_password: "1234"

# grafana_auth:
#   anonymous:
#     org_name: "Main Org."
#     org_role: Viewer

grafana_metrics:
  enabled: true

grafana_datasources:
  - name: "Prometheus"
    type: "prometheus"
    access: "proxy"
    # url: "http://{{ ansible_host }}:9090"
    url: "http://localhost:9090"
    isDefault: true

grafana_dashboards:
  - dashboard_id: 1860
    revision_id: 37
    datasource: prometheus
  - dashboard_id: 9578
    revision_id: 4
    datasource: prometheus

grafana_alerting: {}

alertmanager_web_external_url: "http://{{ ansible_host }}:9093"
alertmanager_receivers:
  - name: telegram_receiver
    telegram_configs:
      - bot_token: "{{ telegram_bot_token }}"
        api_url: https://api.telegram.org
        chat_id: "{{ telegram_chat_id }}"
        parse_mode: ''

alertmanager_route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: telegram_receiver

prometheus_global:
  evaluation_interval: "15s"
  scrape_interval: "15s"
  scrape_timeout: "15s"

prometheus_web_external_url: "http://{{ ansible_host | default(inventory_hostname)  }}:9090"
prometheus_storage_retention: "31d"

prometheus_config_flags_extra:
  enable-feature:
    - promql-at-modifier
    - promql-negative-offset

prometheus_alertmanager_config:
  - scheme: http
    static_configs:
      - targets:
          - "{{ ansible_host | default(inventory_hostname) }}:9093"

prometheus_alert_rules_files:
  - rules/*.yaml
  - rules/*.yml

prometheus_alert_rules:
  - alert: Watchdog
    expr: vector(1)
    for: 10m
    labels:
      severity: warning
    annotations:
      description: 'This is an alert meant to ensure that the entire alerting pipeline is functional.
        This alert is always firing, therefore it should always be firing in Alertmanager
        and always fire against a receiver. There are integrations with various notification
        mechanisms that send a notification when this alert is not firing. For example the
        "DeadMansSnitch" integration in PagerDuty.'
      summary: 'Ensure entire alerting pipeline is functional'
  - alert: NodeCPUUtilizationHigh
    expr: |
      instance:node_cpu_utilisation:rate5m * 100
        > ignoring (severity)
      node_cpu_utilization_percent_threshold{severity="critical"}
    for: 10m
    labels:
      severity: critical
    annotations:
      description: |
        {% raw -%}
        The node CPU utilization ({{ $value }}%) has been over threshold
        ({{ query "node_cpu_utilization_percent_threshold{severity=\"critical\",instance=\"$labels.instance\"}"}}%)
        for 10 minutes.
        {% endraw -%}
      summary: Node CPU Utilization is high

prometheus_targets:
  node:
    - targets:
        "{{ groups['nodes'] | map('extract', hostvars, [prometheus_hostnamelookupkey]) | map('regex_replace', '$', ':9100') | list }}"
      labels:
        env: demo
  alertmanager:
    - targets:
        - "{{ ansible_host | default(inventory_hostname) }}:9093"
      labels:
        env: demo

prometheus_scrape_configs:
  - job_name: "prometheus"
    metrics_path: "/metrics"
    static_configs:
      - targets:
          - "{{ ansible_host | default(inventory_hostname) }}:9090"
  - job_name: "grafana"
    static_configs:
      - targets:
          - "{{ ansible_host | default(inventory_hostname) }}:3000"
  - job_name: "node"
    file_sd_configs:
      - files:
          - "/etc/prometheus/file_sd/node.yml"
  - job_name: "alertmanager"
    file_sd_configs:
      - files:
          - "/etc/prometheus/file_sd/alertmanager.yml"
